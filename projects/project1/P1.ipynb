{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import basic libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from data_utils import *\n",
    "from implementations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample-submission.csv  test.csv               train.csv\r\n"
     ]
    }
   ],
   "source": [
    "# Define some paths\n",
    "root = '../project1/'\n",
    "data = os.path.join(root, 'Data/')\n",
    "train_set_root = os.path.join(data, 'train.csv')\n",
    "test_set_root = os.path.join(data, 'test.csv')\n",
    "%ls data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data preprocessing\n",
    "#### Load datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "          Id Prediction  DER_mass_MMC  DER_mass_transverse_met_lep  \\\n0   100000.0          s       138.470                       51.655   \n1   100001.0          b       160.937                       68.768   \n2   100002.0          b           NaN                      162.172   \n3   100003.0          b       143.905                       81.417   \n4   100004.0          b       175.864                       16.915   \n..       ...        ...           ...                          ...   \n95  100095.0          b        98.181                       68.298   \n96  100096.0          s       113.750                       52.067   \n97  100097.0          s       124.575                       34.558   \n98  100098.0          b        77.578                       72.277   \n99  100099.0          b       107.350                       20.354   \n\n    DER_mass_vis  DER_pt_h  DER_deltaeta_jet_jet  DER_mass_jet_jet  \\\n0         97.827    27.980                 0.910           124.711   \n1        103.235    48.146                   NaN               NaN   \n2        125.953    35.635                   NaN               NaN   \n3         80.943     0.414                   NaN               NaN   \n4        134.805    16.405                   NaN               NaN   \n..           ...       ...                   ...               ...   \n95        77.216     1.170                   NaN               NaN   \n96        76.262     1.250                   NaN               NaN   \n97       100.197   161.051                 3.243           324.597   \n98        58.168   153.606                 2.604           370.937   \n99        77.755    89.044                 4.449           564.836   \n\n    DER_prodeta_jet_jet  DER_deltar_tau_lep  ...  PRI_met_phi  PRI_met_sumet  \\\n0                 2.666               3.064  ...       -0.277        258.733   \n1                   NaN               3.473  ...       -1.916        164.546   \n2                   NaN               3.148  ...       -2.186        260.414   \n3                   NaN               3.310  ...        0.060         86.062   \n4                   NaN               3.891  ...       -0.871         53.131   \n..                  ...                 ...  ...          ...            ...   \n95                  NaN               2.918  ...       -3.011         52.207   \n96                  NaN               2.962  ...        0.234         97.737   \n97               -2.626               1.479  ...       -1.611        432.241   \n98               -1.597               0.988  ...        1.565        351.780   \n99               -2.990               2.482  ...        3.134        327.422   \n\n    PRI_jet_num  PRI_jet_leading_pt  PRI_jet_leading_eta  PRI_jet_leading_phi  \\\n0           2.0              67.435                2.150                0.444   \n1           1.0              46.226                0.725                1.158   \n2           1.0              44.251                2.053               -2.028   \n3           0.0                 NaN                  NaN                  NaN   \n4           0.0                 NaN                  NaN                  NaN   \n..          ...                 ...                  ...                  ...   \n95          0.0                 NaN                  NaN                  NaN   \n96          0.0                 NaN                  NaN                  NaN   \n97          3.0              97.160               -1.686                2.858   \n98          2.0             111.656               -0.987               -0.035   \n99          2.0              95.352               -0.825               -0.201   \n\n    PRI_jet_subleading_pt  PRI_jet_subleading_eta  PRI_jet_subleading_phi  \\\n0                  46.062                   1.240                  -2.475   \n1                     NaN                     NaN                     NaN   \n2                     NaN                     NaN                     NaN   \n3                     NaN                     NaN                     NaN   \n4                     NaN                     NaN                     NaN   \n..                    ...                     ...                     ...   \n95                    NaN                     NaN                     NaN   \n96                    NaN                     NaN                     NaN   \n97                 42.808                   1.557                   1.534   \n98                 92.256                   1.617                  -1.433   \n99                 38.135                   3.625                   2.930   \n\n    PRI_jet_all_pt  \n0          113.497  \n1           46.226  \n2           44.251  \n3            0.000  \n4            0.000  \n..             ...  \n95           0.000  \n96           0.000  \n97         171.763  \n98         203.911  \n99         133.488  \n\n[100 rows x 32 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>Prediction</th>\n      <th>DER_mass_MMC</th>\n      <th>DER_mass_transverse_met_lep</th>\n      <th>DER_mass_vis</th>\n      <th>DER_pt_h</th>\n      <th>DER_deltaeta_jet_jet</th>\n      <th>DER_mass_jet_jet</th>\n      <th>DER_prodeta_jet_jet</th>\n      <th>DER_deltar_tau_lep</th>\n      <th>...</th>\n      <th>PRI_met_phi</th>\n      <th>PRI_met_sumet</th>\n      <th>PRI_jet_num</th>\n      <th>PRI_jet_leading_pt</th>\n      <th>PRI_jet_leading_eta</th>\n      <th>PRI_jet_leading_phi</th>\n      <th>PRI_jet_subleading_pt</th>\n      <th>PRI_jet_subleading_eta</th>\n      <th>PRI_jet_subleading_phi</th>\n      <th>PRI_jet_all_pt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100000.0</td>\n      <td>s</td>\n      <td>138.470</td>\n      <td>51.655</td>\n      <td>97.827</td>\n      <td>27.980</td>\n      <td>0.910</td>\n      <td>124.711</td>\n      <td>2.666</td>\n      <td>3.064</td>\n      <td>...</td>\n      <td>-0.277</td>\n      <td>258.733</td>\n      <td>2.0</td>\n      <td>67.435</td>\n      <td>2.150</td>\n      <td>0.444</td>\n      <td>46.062</td>\n      <td>1.240</td>\n      <td>-2.475</td>\n      <td>113.497</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>100001.0</td>\n      <td>b</td>\n      <td>160.937</td>\n      <td>68.768</td>\n      <td>103.235</td>\n      <td>48.146</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3.473</td>\n      <td>...</td>\n      <td>-1.916</td>\n      <td>164.546</td>\n      <td>1.0</td>\n      <td>46.226</td>\n      <td>0.725</td>\n      <td>1.158</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>46.226</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>100002.0</td>\n      <td>b</td>\n      <td>NaN</td>\n      <td>162.172</td>\n      <td>125.953</td>\n      <td>35.635</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3.148</td>\n      <td>...</td>\n      <td>-2.186</td>\n      <td>260.414</td>\n      <td>1.0</td>\n      <td>44.251</td>\n      <td>2.053</td>\n      <td>-2.028</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>44.251</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>100003.0</td>\n      <td>b</td>\n      <td>143.905</td>\n      <td>81.417</td>\n      <td>80.943</td>\n      <td>0.414</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3.310</td>\n      <td>...</td>\n      <td>0.060</td>\n      <td>86.062</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>100004.0</td>\n      <td>b</td>\n      <td>175.864</td>\n      <td>16.915</td>\n      <td>134.805</td>\n      <td>16.405</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3.891</td>\n      <td>...</td>\n      <td>-0.871</td>\n      <td>53.131</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>100095.0</td>\n      <td>b</td>\n      <td>98.181</td>\n      <td>68.298</td>\n      <td>77.216</td>\n      <td>1.170</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2.918</td>\n      <td>...</td>\n      <td>-3.011</td>\n      <td>52.207</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>100096.0</td>\n      <td>s</td>\n      <td>113.750</td>\n      <td>52.067</td>\n      <td>76.262</td>\n      <td>1.250</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2.962</td>\n      <td>...</td>\n      <td>0.234</td>\n      <td>97.737</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>100097.0</td>\n      <td>s</td>\n      <td>124.575</td>\n      <td>34.558</td>\n      <td>100.197</td>\n      <td>161.051</td>\n      <td>3.243</td>\n      <td>324.597</td>\n      <td>-2.626</td>\n      <td>1.479</td>\n      <td>...</td>\n      <td>-1.611</td>\n      <td>432.241</td>\n      <td>3.0</td>\n      <td>97.160</td>\n      <td>-1.686</td>\n      <td>2.858</td>\n      <td>42.808</td>\n      <td>1.557</td>\n      <td>1.534</td>\n      <td>171.763</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>100098.0</td>\n      <td>b</td>\n      <td>77.578</td>\n      <td>72.277</td>\n      <td>58.168</td>\n      <td>153.606</td>\n      <td>2.604</td>\n      <td>370.937</td>\n      <td>-1.597</td>\n      <td>0.988</td>\n      <td>...</td>\n      <td>1.565</td>\n      <td>351.780</td>\n      <td>2.0</td>\n      <td>111.656</td>\n      <td>-0.987</td>\n      <td>-0.035</td>\n      <td>92.256</td>\n      <td>1.617</td>\n      <td>-1.433</td>\n      <td>203.911</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>100099.0</td>\n      <td>b</td>\n      <td>107.350</td>\n      <td>20.354</td>\n      <td>77.755</td>\n      <td>89.044</td>\n      <td>4.449</td>\n      <td>564.836</td>\n      <td>-2.990</td>\n      <td>2.482</td>\n      <td>...</td>\n      <td>3.134</td>\n      <td>327.422</td>\n      <td>2.0</td>\n      <td>95.352</td>\n      <td>-0.825</td>\n      <td>-0.201</td>\n      <td>38.135</td>\n      <td>3.625</td>\n      <td>2.930</td>\n      <td>133.488</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows Ã— 32 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(train_set_root)\n",
    "for col_name in data.columns:\n",
    "     data.loc[data[col_name] == -999.0,col_name]= np.nan\n",
    "# data = data.dropna(how='any', axis=0)\n",
    "data.head(100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "(array([99913.,     0.,     0., 77544.,     0.,     0., 50379.,     0.,\n            0., 22164.]),\n array([0. , 0.3, 0.6, 0.9, 1.2, 1.5, 1.8, 2.1, 2.4, 2.7, 3. ]),\n <BarContainer object of 10 artists>)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(data['PRI_jet_num'])# data imbalance"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Begins here"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "train_data, train_label = load_data(train_set_root)\n",
    "train_data = standardize(train_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "x = train_data.copy()\n",
    "feature_idx_dict = {0: [0, 1, 2, 3, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21],\n",
    "                        1: [0, 1, 2, 3, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 29],\n",
    "                        2: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25,\n",
    "                            26, 27, 28, 29]}\n",
    "jetnum = 22\n",
    "\n",
    "group0 = x[np.where(x[:, jetnum] == 0)]\n",
    "group0_x = group0[:, feature_idx_dict[0]]\n",
    "\n",
    "group1 = x[np.where(x[:, jetnum] == 1)]\n",
    "group1_x = group1[:, feature_idx_dict[1]]\n",
    "\n",
    "group2 = x[np.where(x[:, jetnum] >= 2)]\n",
    "group2_x = group2[:, feature_idx_dict[2]]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "array([], shape=(0, 30), dtype=float64)"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 1., -1., -1., ...,  1., -1., -1.])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import data_utils\n",
    "def cross_validation(train_data, train_label, k_fold, eval_func, input_dict):\n",
    "  data_num = train_data.shape[0]\n",
    "  data_dim = train_data.shape[1]\n",
    "  fold_part_idx = data_utils.k_fold(data_num, k_fold)\n",
    "\n",
    "  loss_list = []\n",
    "  acc_list = []\n",
    "  for k in range(k_fold):\n",
    "    cur_train_idx = np.concatenate([fold_part_idx[:k, :], fold_part_idx[k+1:, :]], axis = 0)\n",
    "    cur_train_idx = cur_train_idx.flatten()\n",
    "    cur_test_idx = fold_part_idx[k, :]\n",
    "    x_tr = train_data[cur_train_idx, :]\n",
    "    x_te = train_data[cur_test_idx, :]\n",
    "    y_tr = train_label[cur_train_idx]\n",
    "    # print(np.sum(y_tr==1)/len(y_tr))\n",
    "    y_te = train_label[cur_test_idx]\n",
    "\n",
    "    lambda_ = input_dict.get('lambda_')\n",
    "    initial_w = input_dict.get('initial_w')\n",
    "    max_iters = input_dict.get('max_iters')\n",
    "    gamma = input_dict.get('gamma')\n",
    "    sgd = input_dict.get('sgd')\n",
    "    func_type = input_dict.get('func_type')\n",
    "\n",
    "    w, loss = eval_func(y_tr, x_tr, lambda_, initial_w, max_iters, gamma, sgd)\n",
    "    _, train_acc = compute_statistics_all(y_tr, x_tr, w, func_type = input_dict['func_type'])\n",
    "    test_loss, test_acc = compute_statistics_all(y_te, x_te, w, func_type = input_dict['func_type'])\n",
    "    loss_list.append(test_loss)\n",
    "    acc_list.append(test_acc)\n",
    "  return np.mean(loss_list), np.mean(acc_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The cross validation part"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test for the ```reg_logistic_regression```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "lambdas = np.logspace(-10, 0, 10)\n",
    "gammas = [0.015]\n",
    "iters = [1000]\n",
    "test_func = reg_logistic_regression\n",
    "cleans = [0, 1]\n",
    "polys = [1]\n",
    "degrees = [9]\n",
    "normalizes = [0, 1]\n",
    "all_polys = [True, False]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def compute_statistics_all(y, tx, w, func_type='linear'):\n",
    "    \"\"\"Compute Loss and Accuracy after training.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy arrays of shape (2, ), final weight\n",
    "        func_type: type of model used, linear for linear regression and logistic for logistic regression.\n",
    "\n",
    "    Returns:\n",
    "        loss: scalar\n",
    "        acc: classification accuracy\n",
    "    \"\"\"\n",
    "    data_num = y.shape[0]\n",
    "\n",
    "    if (func_type == 'linear'):\n",
    "        model_output = np.dot(tx, w)\n",
    "        e_mat = y - model_output\n",
    "        loss = 0.5 * np.dot(e_mat.T, e_mat) / data_num\n",
    "        model_output[np.where(model_output > 0)] = 1\n",
    "        model_output[np.where(model_output <= 0)] = -1\n",
    "    elif (func_type == 'logistic'):\n",
    "        model_output = sigmoid(np.dot(tx, w))\n",
    "        loss = -np.dot(y, np.log(model_output + eps)) - np.dot((1 - y).T, np.log(1 - model_output + eps))\n",
    "        model_output[np.where(model_output > 0.5)] = 1\n",
    "        model_output[np.where(model_output <= 0.5)] = 0\n",
    "\n",
    "    acc = np.sum(model_output == y) / data_num\n",
    "    return loss, acc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "k_fold_num = 5\n",
    "group_num = 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "train_label_reg = train_label.copy()\n",
    "train_label_reg[train_label_reg == -1] = 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "lambdas = np.logspace(-10, 0, 10)\n",
    "gammas = [0.05]\n",
    "max_iters = [1000]\n",
    "test_func = reg_logistic_regression\n",
    "cleans = [0, 1]\n",
    "polys = [1]\n",
    "degrees = [12,11,10,9,8]\n",
    "normalizes = [0, 1]\n",
    "all_polys = [True, False]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "linear regression gd/sgd      try gamma try max_iter  try degree\n",
    "ridge regression: lambda      try degree\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "num_iter_ = 0\n",
    "group_num = 0\n",
    "max_acc = 0\n",
    "print(train_label_reg[:5])\n",
    "for clean in cleans:\n",
    "  for poly in polys:\n",
    "    if poly:\n",
    "      for gamma in gammas:\n",
    "        for degree in degrees:\n",
    "          degree_list = [1] * 3\n",
    "          degree_list[group_num] = degree\n",
    "          for normalize_ in normalizes:\n",
    "            for all_poly in all_polys:\n",
    "              group0_x, group0_labels, group1_x, group1_labels, group2_x, group2_labels = \\\n",
    "                            data_utils.process_data(train_data, train_label_reg, clean = clean)\n",
    "              group0_x, group1_x, group2_x = \\\n",
    "                      data_utils.group_poly(group0_x, group1_x, group2_x, degree_list, all_poly)\n",
    "              cur_group = [group0_x, group1_x, group2_x]\n",
    "              cur_labels = [group0_labels, group1_labels, group2_labels]\n",
    "              group_x = cur_group[group_num]\n",
    "              group_label = cur_labels[group_num]\n",
    "              if(normalize_):\n",
    "                group_x = data_utils.normalize(group_x)\n",
    "\n",
    "              input_dict = {'lambda_':0, \\\n",
    "                      'initial_w':np.zeros(group_x.shape[1]), \\\n",
    "                      'max_iters':8000, \\\n",
    "                      'gamma':gamma, \\\n",
    "                      'sgd':False, \\\n",
    "                      'func_type':'logistic'}\n",
    "              loss, acc = cross_validation(group_x, group_label, k_fold_num, test_func, input_dict)\n",
    "              print(f'For this combination scheme.\\iter:{num_iter_} \\\n",
    "                    Clean:{clean}, \\\n",
    "                    Poly:{poly}, \\\n",
    "                    Degree:{degree}, \\\n",
    "                    Normalize: {normalize_}, \\\n",
    "                    all_poly:{all_poly}, \\\n",
    "                    Acc:{acc}, Gamma:{gamma}')\n",
    "              if(acc > max_acc):\n",
    "                print(f'Found a better scheme.\\ iter:{num_iter_} \\ Clean:{clean}, \\\n",
    "                    Poly:{poly}, \\\n",
    "                    Degree:{degree}, \\\n",
    "                    Normalize: {normalize_}, \\\n",
    "                    all_poly:{all_poly}, \\\n",
    "                    Acc:{acc}, Gamma:{gamma}')\n",
    "                max_acc = acc\n",
    "              num_iter_ = num_iter_ + 1\n",
    "    else:\n",
    "      for gamma in gammas:\n",
    "        for normalize_ in normalizes:\n",
    "          group0_x, group0_labels, group1_x, group1_labels, group2_x, group2_labels = \\\n",
    "                        data_utils.process_data(train_data, train_label_reg, clean = clean)\n",
    "          cur_group = [group0_x, group1_x, group2_x]\n",
    "          cur_labels = [group0_labels, group1_labels, group2_labels]\n",
    "          group_x = cur_group[group_num]\n",
    "          group_label = cur_labels[group_num]\n",
    "          if(normalize_):\n",
    "            group_x = data_utils.normalize(group_x)\n",
    "          input_dict = {'lambda_':0, \\\n",
    "                  'initial_w':np.zeros(group_x.shape[1]), \\\n",
    "                  'max_iters':10000, \\\n",
    "                  'gamma':gamma, \\\n",
    "                  'sgd':False, \\\n",
    "                  'func_type':'logistic'}\n",
    "          try:\n",
    "            loss, acc = cross_validation(group_x, group_label, k_fold_num, test_func, input_dict)\n",
    "          except:\n",
    "            print(f'Wrong. \\\n",
    "                Clean:{clean}, \\\n",
    "                Poly:{poly}, \\\n",
    "                Normalize: {normalize_}')\n",
    "            continue\n",
    "          print(f'For this combination scheme.\\iter:{num_iter_} \\\n",
    "                  Clean:{clean}, \\\n",
    "                  Poly:{poly}, \\\n",
    "                  Degree:{degree}, \\\n",
    "                  Normalize: {normalize_}, \\\n",
    "                  all_poly:{all_poly}, \\\n",
    "                  Acc:{acc}, Gamma:{gamma}')\n",
    "          if(acc > max_acc):\n",
    "            print(f'Found a better scheme. \\iter:{num_iter_} \\\n",
    "                Clean:{clean}, \\\n",
    "                Poly:{poly}, \\\n",
    "                Normalize: {normalize_}, \\\n",
    "                Acc:{acc}, gamma:{gamma}')\n",
    "            max_acc = acc\n",
    "          num_iter_ = num_iter_ + 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}